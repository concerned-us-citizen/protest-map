name: Build & Deploy to GitHub Pages

on:
  # push:
  #   branches: [main]
  # schedule:
  #   - cron: "29 * * * *"
  workflow_dispatch:

permissions:
  contents: read
  pages: write
  id-token: write

concurrency:
  group: pages
  cancel-in-progress: true

env:
  BASE_PATH: "/${{ github.event.repository.name }}"
  GITHUB_USERNAME: ${{ github.repository_owner }}
  SPATIAL_INDEX_TAG: "precincts-v1" # GitHub release tag

jobs:
  deploy:
    runs-on: ubuntu-latest
    timeout-minutes: 120
    environment:
      name: github-pages

    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        # LFS removed

      - name: Set up Node.js
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: npm

      - name: Install dependencies
        run: npm install --legacy-peer-deps

      - name: Download latest precincts spatial index from GitHub Release
        env:
          GH_REPO: ${{ github.repository }}
        run: |
          mkdir -p cache

          # Get latest release tag
          TAG=$(curl -s https://api.github.com/repos/$GH_REPO/releases/latest | jq -r .tag_name)

          echo "Latest release tag: $TAG"

          # Download file
          curl -L -o cache/precincts-with-results-spatial-index.json \
            https://github.com/$GH_REPO/releases/download/$TAG/precincts-with-results-spatial-index.json

          # Ensure file was downloaded and is not empty
          test -s cache/precincts-with-results-spatial-index.json || {
            echo "❌ Failed to download or file is empty!"
            exit 1
          }

      - name: Restore cached scrape data if available
        uses: actions/cache@v4
        with:
          path: cache
          key: ${{ runner.os }}-scrape-cache-${{ hashFiles('prebuilt_data/*.json') }}
          restore-keys: ${{ runner.os }}-scrape-cache-

      - name: Initialize cache with prebuilt data
        run: |
          mkdir -p cache
          for f in geocache.json bad_geocache.json wikicache.json; do
            # Check if prebuilt_data file exists
            if [ -e "prebuilt_data/$f" ]; then
              # Check if cache file exists AND prebuilt_data file is NOT newer than cache file
              if [ -e "cache/$f" ] && [ ! "prebuilt_data/$f" -nt "cache/$f" ]; then
                echo "Cache file cache/$f is newer or same age as prebuilt_data/$f, keeping cache version."
              else
                # If cache file doesn't exist OR prebuilt_data file is newer than cache file
                cp "prebuilt_data/$f" "cache/$f"
                echo "Copied prebuilt_data/$f (newer or cache did not exist)."
              fi
            fi
          done

      - name: Run scraper – generates/updates cache/*.json
        run: npm run scrape

      - name: Save updated scrape cache
        uses: actions/cache@v4
        with:
          path: cache
          key: ${{ runner.os }}-scrape-cache-${{ hashFiles('prebuilt_data/*.json') }}

      - name: Build static site
        run: npm run build

      - name: Upload Pages artifact
        uses: actions/upload-pages-artifact@v3
        with:
          path: build

      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4
